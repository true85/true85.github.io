---
title:  "ML 기초"
layout: single
categories: code
tag: ML
toc: true
use_math: true #수학식 적용여부
author_profile: false #내 프로파일 안보이기
sidebar:
    nav: "docs" 
# search: false #검색불가
---
## 데이터
### 정형데이터 
: 숫자
### 비정형 데이터
#### 텍스트
#### 이미지
눈은 색보다 밝기에 민감하다.
jpg는 용량을 줄이기 위해 밝기 정보와 Cb Cr이라는 두가지 색차 정보로 저장된다.
사진은 PNG가 좋다.

JPEG, JPG (Joint Photographic Experts Group)
: 정지 화상을 위해서 만들어진 손실 압축 방법 표준이다. 
(JPEG 를 사용하는 파일 형식들도 보통 JPEG 이미지라 불리며, .jpg, .jpeg, .jpe 등의 확장자를 사용한다. 손실 압축 형식이지만 파일크기가 작기 때문에 웹에서 널리 쓰인다. 압축률을 높이면 파일 크기는 작아지지만 이미지 품질은 더욱 떨어진다.)

PNG(Portable Network Graphics)
: 비손실 그래픽 파일 포맷, 투명성 값도 포함
(무손실 압축으로 이미지 디테일 손실이 전혀없고 고품질 이미지를 생성하지만 파일 크기는 상대적으로 다른 포맷보다 커진다.)

GIF(Graphics Interchange Format)
: PNG 포맷 이전에 개발된 비손실 그래픽 포맷 중의 하나이다.
(1개의 파일에 여러 개의 이미지를 저장할 수 있는데, 다수의 이미지를 하나의 이미지처럼 복수 처리하여 간단한 애니메이션 효과를 낼 수 있다. 웹 페이지 상에서 움직이는 그림(움짤)은 GIF 를 사용한다. )

#### 영상
코덱(Codec) = Coder + Decoder 
압축, 변환과정 Encoding 
##### 디스플레이
물감은 감산혼합 잉크
디스플레이는 가산혼합 빛
색 재현율
: 화면에서 나타낼수있는 색의 범위 , sRGB/Adobe RGB(프린터를 위한)/ DCI-P3(영화를 위한)
##### 해상도
화소
: 색을 나타내는 점
화질 = 해상도 + 명암비 + 색표현
16:9
1080p (ex. 1920*1080, 순차주사 Progressive scan)
1080i (ex. 2560*1080, 비월 주사 Interlaced sacn)
720p HD
1080p Full HD
1440p Quad HD
2160p Ultra HD
#### 음성

## 인공지능
사고나 학습등 인간이 가진 지적 능력을 컴퓨터를 통해 구현하는 기술

## 머신러닝
컴퓨터가 스스로 학습하여 인공지능의 성능을 향상시키는 기술 방법


### 지도학습(supervised learning)
: 정답 라벨로 학습

#### 회귀 (regression) 
: 유전체 선발, 시장 예측 - Random forest, Regression, 정규화 (regularization)
##### 회귀 평가 방법
- 평균제곱오차 (mean square error)
: 각 값의 오차를 제곱하여 평균한 값
- 결정계수 (coefficient of determination)
: 0~1사이 1은 오차가 없음

##### 선형회귀(linear regression)
손실을 최소화하는 파라미터 계산
$y = wx + b$
평균제곱오차 비교 : **각 데이터마다 y값 오차를 제곱하여 데이터갯수로 나누어 평균**을 해서 나온값을 비교
데이터 분포를 시각화 해서 적용가능한지 먼저 판단

**a,b로 정리하면 다항2차 방정식이** 나오는데 이것으로 최소값을 찾는다.

##### 정규화 (regularization)
과적합을 막는 방법중 하나

[손실함수](https://truman.tistory.com/164)
- 손실 함수는 실제값과 예측값의 차이(loss, cost)를 수치화해주는 함수이다. 
- 오차가 클수록 손실 함수의 값이 크고, 오차가 작을수록 손실 함수의 값이 작아진다.
- 평균제곱오차도 하나의 손실함수

정규화 항($\alpha$)을 조정하여 절댓값차이를 줄임

#### 분류 (classification) 
: 패턴 인식, 의학 진단 -  k-nn(K-nearest neighbor), SVM (Support Vector Machine), Logistic regression, Naive Bayes, Decision Tree
##### 분류 평가 방법
- 혼동 행렬 (confusion matrix)
: TN 실제 부정 데이터를 부정으로 제대로 예측, FP 실제 부정 데이터를 긍정으로 잘못 예측, FN 실제 긍정 데이터를 부정으로 잘못 예측, TP 실제 긍정 데이터를 긍정으로 제대로 예측 (행렬표시)
- 정확도 (accuracy)
: 전체 결과 중 정확하게 예측한 비율
- 정밀도 (precision)
: 실제 긍정으로 예측한 결과 중 정말 긍정인 비율
- 재현율 (recall)
: 실제 긍정인 대상을 긍정으로 제대로 예측한 비율
- F값 (F1-score)
정밀도와 재현율을 반영한 지표
- 곡선아래 면적 (area under the curve, AUC)
##### 로지스틱 회귀 (Logistic regression)
집단 각각에 속하는 확률 계산

이진분류는 시그모이드함수 활용(임계값 기준으로 1혹은 0) 또는 결정경계

손실함수로는 주로 경사하강법 이용

##### Naive Bayes
자연어 분류에 많이 이용되는 알고리즘
확율에 따른 결과를 예측하는 알고리즘
문장속 단어가 나타나는 비율, 단어 각각의 조건부 확률 등 문장을 구성하는 단어 정보를 사용해 확률 계산의 정확도를 높인다.
독립사건의 영향을 다룰때 사용한다.

##### 결정트리 (Decision Tree)
학습데이터를 나누면서 문제를 해결
데이터의 불균형을 나타내는 불순도를 수치화

앞의 두과정을 반복

#### 회귀, 분류 둘다가능한 알고리즘
SVM (Support Vector Machine)/- with kernel, random forest,K-nn(K-nearest neighbor), neural network

##### SVM (Support Vector Machine)
집단 데이터 사이의 마진을 최대화해서 좋은 결정경계를 구한다.

하드마진 : 마진안에 데이터 허용X
소프트마진 : 마진안에 데이터 허용

마진선에 있는 데이터와 안에있는 데이터를 기준으로 경계를 정한다.
이 기준 파라미터 정하는 방식으로 그리드 탐색과 랜덤 탐색이 있다.

##### SVM (Support Vector Machine) + 커널 kernel
선이 아니더라도 차원을 늘려서 경계를 만들수있다.(선형, 시그모이드, 다항(원형태), RBF 커널)

##### 랜덤 포레스트 (random forest)
결정트리를 어러번 중첩시켜 사용 다수결로 결과를 도출
다른 다양한 결과를 얻기 위해
1. 부트스트랩(bootstrap)
: 학습 데이터에서 여러번 무작위로 복원(반복) 추출
1. 특징 임의 선택
: 부트스트랩으로 만든 학습데이터에 일부 특징만을 임의로 선택 사용

##### 신경망 (neural network)
분류에 많이 사용된다.
생물의 신경망을 따서 만듬, 딥러닝에 활용
입력층 은닉층 출력층 
$y=f(w_0+w_1x_1+w_2x_2)$
$w_1,w_2$는 가중치 $w_0$는 편향 bias 를 활성화 함수($f$)에 적용 
과적합을 막기위해 학습조기종료

##### k-최근접 이웃 알고리즘(kNN, K-nearest neighbor)
학습데이트를 저장하여 입력데이터와의 거리를 계산하여 근처에 있는 k개의 점까지 데이터를 어떻게 분류하는지를 보고다수결로 결정

### 비지도 학습(unsupervised learning) 
: 정답없이 학습

#### 군집 (clustering) 
: 비슷한 성질릐 데이터를 집단으로 나누는 방법 
시장 세분화/ 추천자 시스템 - k-means, 가우시안 혼합모델(Gaussian Mixture models), Kierarchical clistering
##### k-평균알고리즘 (k-means)
1. 데이터 포인트 중 적당한 점을 집단 개수 만큼 선택해 중심으로 정함
1. 데이터 포인트와 각 궂ㅇ심 사이의 거리를 계산해 가장 가까운 중심을 해당 데이터 포인트가 속한 집단으로 정함
1. 집단 마다 데이터 포인트의 평균을 계싼하고 이를 새로운 중심으로 정함
1. 데이터 포인트 모다가 속한 집단이 변하지 않거나 더는 계산할 수 없을 때까지 반복

집단 안 제곱합으로 결과를 평가 합니다.
##### 가우시안 혼합모델(Gaussian Mixture models)
분산을 이용해 데이터 분포를 나타냄
1. 파라미터(가우스 분포 각각의 평군과 분산) 초기화
1. 데이터 포인트 각각이 갖는 가중치를 집단 마다 계산
1. 과정 2에서 얻은 가중치로 파라미터를 다시 계산
1. 과정 3에서 갱신한 평군 각각의 변화가 충분히 작아질 때까지 2,3 반복

#### 차원축소 (dimensionality reduction) 
: 특징 추출/ 빅데이터 시각화 - 주성분 분석(PCA, principal component analysis), 잠재 의미분석(LSA, latent semantic analysis), 음수 미포함 행렬 분해(NMF, non-negative matrix factorization), 잠재 디리클레 할당(LDA, latent Dirichlet allocation), 국소 선형 임베딩(LLE, local linear embedding), t-분포 확률적 임베딩(t-SNE, t-distributed stochastic neighbor embedding)

##### 주성분 분석 (PCA, principal component analysis)
상관관계가 있는 다변량 데이터를 주성분으로 간경하게 나타낼 수있다.

원본 데이터를 가공해 새로운 변수로 구성

방향과 중요도를 찾아서 가장 큰값을 주성분으로 잡는다. 두번쩨 큰값을 두번쩨 주성분으로 잡는다.

##### 잠재 의미 분석 (LSA, latent semantic analysis)
토픽 모델링 알고리즘

특이값 분해(Singular Value Decomposition, SVD)

SVD(full SVD)

절단된 SVD(Truncated SVD)

$X=UDV$

U : 단어특징의 변화 정보가 있는 행렬

D : 정보의 중요도가 있는 행렬

V : 단어 특징과 문서 변환 정보가 있는 행렬

##### 잠재 디리클레 할당(LDA, latent Dirichlet allocation)
토픽 모델링 알고리즘
1. 문서에 사용할 단어의 개수 k를 정합니다.
1. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정합니다.
1. 문서에 사영할 각 단어를 다음과 같이 정합니다.
    1. 토픽 분포에서 토픽 T를 확률적으로 고릅니다.
    1. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고릅니다.

불용어를 제외하도록 학습을 개선 가능.
##### 음수 미포함 행렬 분해(NMF, non-negative matrix factorization)
입력 데이터와 출력 데이터값이 모두 양수라는 성질이 있는 차원 축소 방법

##### 국소 선형 임베딩(LLE, local linear embedding)
높은 차워 공간에서 휘어지거나 뒤틀린 구조를 낮은 차원 공간에 단순한 구조로 나타내는 알고리즘

##### t-분포 확률적 임베딩(t-SNE, t-distributed stochastic neighbor embedding)
높은 차원의 복잡한 데이터를 2차원에 차원 축소하는 방법
### 강화 학습 (reinforcement learning)
: 시뮬레이션 반복학습, 로봇탐색/ 인공지능 게임 - K-armed bandit, 모델 기반 학습, 모델 프리 학습, 가치 함수 근사, 이미테이션 러닝

## 딥러닝
인간의 뉴런과 비슷한 인공신경망 방식으로 정보를 처리 (비지도 학습)

퍼셉트론에서 나옴 -> 다층 퍼셉트론 MLP

순전파후 (앞으로 진행) 결과 오차값을 가지고 역전파를 (뒤로 진행)이용해서 각 가중치 값을 수정하여 오류를 죽인다. (반복)

하이퍼파라미터로 층수와 노드수 
### 원핫 인코딩

하나의 값만 1 나머지 0인 행렬

### 활성화 함수
앞단의 신호의 합을 사용할지 말지 (ex: 1,0) 결정하는 함수

선형함수라면 도함수값이 항상 상수라 나중에 역전파에서 문제가 생긴다.

- step : 일정값을 기준으로 0과 1로 구분, 이진 분류문제에 자주 이용
- sigmoid/ logistic : 시그모이드는 이진분류에서 두클래스의 확률을 구할때 자주 이용된다. 무한의 연속 변수를 0~1사이의 값으로 반환한다.
- 소프트맥스 (출력 단에 활용) : 함수의 모든 출력의 합은 1, 각 값은 0~1 사이이다.
- tanh 함수 : 시그모이드 이동 버전으로 값은 -1~1사이 이다.
- ReLU : 시그모이드나 tanh함수는 극단값의 기울기가 매우 작아진다는 단점을 해결하기 위해나온, 기준 이상이면 0 그외는 선형 일차 함수
- 누설 ReLU : 렐루의 단점 음수인 경우 기울기가 0. 음수에도 약간의 기울기(0.01)를 줌

### 오차 함수
: 결과가 실제와 얼마나 다른지 측증하는 함수
- 평규제곱오차
- 교차 엔트로피 : 확률분포사이에 오차 측정

### 최적화 기법
: 오차가 최소가 되는 가중치를 찾는 역할 

![참조 그림](https://github.com/true85/true85.github.io/blob/master/_posts/img/adam.png?raw=true)

눈을 가리고 계곡 밑으로 내려가는 법(왔는 길만 기억한다.)

- 경사하강법(SGD) 
    - 배치경사하강법 : 배치를 전체 데이터로 둔다. 지역 극소점에서 멈출수있다. 매번 훈련 데이터 전체를 사용한다.
    - 확률적 경사 하강법 : 매개변수 값 조정 시 전체가 아니라 랜텀으로 선택한 하나의 데이터에 대해서만 계산. 지그제그로 경사를 내려간다.
    - 미니 배치경사하강법 : 정해진 데이터 양에 대해서만 계산하여 매개변수 값 조정. 가장 많이 사용된다. 
- 모멘텀
- 아다그라드
- RMSProp
- Adam

### 배치 크기
: 파라미터를 한번 업데이트할 때마다 신경망에 입력되는 학습 데이터 수. 배치 크키가 클수록 학습시간은 짧아 지지만, 메모리 용량이 많이 필요하다 기본추천 32->64->128->256
### 에포크 수
: 학습중 전체 학습 데이터가 신경망에 입력되는 횟수를 의미, 정확도가 떨어질때가지 반복 추천
### 학습률
: 최적화 알고리즘의 파라미터. 낮은 값이면 정확한 값을 찾지만 시간이 걸리고, 큰값이면 속도가 빨라지나 정확한 답을 찾지 못할수 있다.
### 과적합 피하기
- 학습셋과 테스트셋으로 분리해서 과적합인지 검증
- k 교차 검증 : 데이터셋을 k개로 나누어 학습과 테스트를 교차해서 진행
- 학습의 자동중단 : 테스트셋 오차가 줄지 않으면 학습 정지
- 드롭아웃 : 무작위로 뉴런을 제거
- 가중치 감소 : 가중치 증가에 벌점 부가

### CNN (Convolutional Neural Network) 합성곱 신경망
MLP 가 백터 값으로 입력받아야하는 단점(공간적 특징이 사라짐)을 해결한 방법
작은 특징에서 큰 특징으로 확장
필터 혹은 커널을 가지고 합성곱층 만듬 -> 렐루 -> 풀링 ...(합/활/풀 반복)... -> 백터로 변환 -> (드롭아웃층) -> 전결합층 ...(드/전 반복)...-> 소프트맥스

합성곱층
: 커널이 이미지를 순환하며 특징을 추출한다. 그 결과를 특징맵 혹은 활성화 맵이라고 부른다.

커널값
: 합성곱행렬 가중치(무작위 값 초기화) 학습되는 값

스트라이드와 패딩
: 필터가 지나가는 스탭의 크기와 줄어든 사이즈 외곽를 채우는 것

풀링층
: 전달되는 파라미터를 줄이는 층(평균혹은 통계함수를 이용)
- 최대풀링 : 층에서 큰값을 가지고 작은 층으로 만든다.
- 평균풀링 : 층의 값의 평균을 가지고 작은 층을 만든다.

### RNN (Recurrent Neural Network) 순환 신경망
앞의 값을 영향을 받아 전체적인 답을 찾아낸다.(은닉층을 순환한다.)

단점으로 가중치를 중복사용으로 기울기 소실이나 폭발이 발생한다.
#### Seq2Seq

### LSTM (Long Short Term Memory) 
장기적인 기억을(기억셀) 유지하고, 기울기 소실 문제도 해결한 모델 (RNN 에서 발전된)

### GRU (Gated Recurrent Unit)
LSTM 모델의 단순화 (리셋 게이트)

### VAE (Variational Autoencoder)
이미지 등의 특징을 잠재 변수로 압축한 후 이를 조정하면서 생성하는 이미지를 연속해서 바꿀 수 있습니다.

#### Conditional VAE
잠재 변수 뿐만 아니라 레이블도 디코더에 입력하여 레이블을 지정하는 형태로 데이터를 생성합니다.

#### ß-VAE (disentanglement)
이미지의 특징을 잠재 공간에서 분리하는 응용 기술

#### VQ-VAE(-2) (Vector Quantised)
해상도를 올려준다.

### GAN (Generative adversarial networks) 생성적 적대 신경망
생성자와 식별자가 서로 경쟁하여 모델을 발전 시킨다.

생성자(Generator)
: 가짜 데이터를 만드는 모델로, 식별자를 속인다. 랜덤 노이즈를 입력해 가짜 데이터를 만든다.

식별자 (Discriminator)
: 생성자가 만든 가짜를 식별한다. 원본 데이터와 생성자가 만든 데이터 모두 훈련 데이터로 삼아 아주 작은 차이도 식별하도록 훈련.

#### GAN 용도
얼굴 이미지 합성(ex. 미소짓는 여성 - 무표정 여성 + 무표정한 남성= 미소 짓는 남성)

#### DCGAN (CNN + GAN)

#### pix2pix

#### Cycle GAN

### RBM (Restricted Boltzmann Machine)

### 사물탐지
#### 4요소
- 영역 제안 : 이미지내에 시스템이 처리할 영역 찾기 (RoI : regions of interest) 여러 세분화 영역의 유사도가 비슷한 것을 합쳐 하나(실루엣)가 될때 까지 반복한다.
- 특징 추출 및 예측 : 찾은 객체 예측
- 비최대 억제 : 한 객체에 하나의 영역 선택 (NMS : non-maximum suppression)
- 평가지표 
    - 초당 처리 프레임 수
    - 평균평균정밀도 (mAP : mean average precision)
    - PR 곡선 (precision-recall curve)
    - 중첩률(IoU : intersection iver union) : 충첩되는 박스의 비율
#### Model
- R-CNN
- SSD
- YOLO

### 전이 학습
기존에 학습된 신경망을 활용하는 방법
- 데이터 부족한 경우
- 과다한 계산을 요구하는 경우

> 특징 추출부를 이용하고, 분류부분(FC)는 제거한다.

`from keras.applications.vgg16 import VGG16`
`base_model = VGG16(weights = "imagenet", include_top=False, input_shape=(224,224,3))`
include_top=False 분류기 가중치를 내려받지 않음 
```py
for layer in base_model.layers:
    layer.traninable = False # 기존 층의 가중치를 고정
```

```py
last_layer = base_model.get_layer('block5_pool') # 마지막층에 접근
last_output = last_layer.output # 마지막 층의 출력을 다음층의 입력으로 심는다.
x = Flatten()(last_output)
x = Dense(2, activation = 'softmax', name = 'softmax')(x) #유닛이 2개인 새 소프트맥스층을 모델에 추가
```
?소프트맥스만 추가?
## 데이터 세트

### 사이킷런 데이터 세트
- 보스턴 주택 가격
- iris
- 당뇨병
- 숫자 손글씨
- 운동능력
- 와인
- 유방암

### 케라스
- CIFAR10(0) 10(0)개의 이미지셋
- IMDB 영화 리뷰
- 로이터 뉴스
- MNIST 숫자 손글씨
- Fashion-MNIST

## 특성선택과 특성공학

### 특성선택
: 초기 데이터 세트에서 특성의 하위 집합을 선택하는 데 사용하는 방법

- 학습시간 단축
- 단순화 해서 해석을 쉽게
- 과적합을 줄여 성능 향상

#### 특성선택 방법
- 특성 중요도 : ExtraTreesClassifier()
- 일 변량 선택 : SelectKBest()
- 상관관계 히트맵 : sns.heatmap()
- 래퍼 기반 방법 : 여러번에 계산으로 특성 추가 혹은 제거, 많은 시간과 비용이 소모된다.
    - 포워드 선택(forward selection) : 특성이 없는 상태에서 특성을 추가하여 성능이 올라가면 유지, 그렇지 않으면 폐기하는 방식
    - 백워드 제거(backward elimination) :  모든 특성을 가진상태로 시작해서 중요도가 낮은 특성을 제거하고 성능을 비교, 큰 개선이 없을 때까지 반복
    - 재귀적 특성 제거(recursive feature elimination) : 탐욕적 최적화 알고리즘으로, 여르개의 모델을 생성하여 비교
- 필터 기반 방법
    - 피어선 상관관계(Pearson's correlation) : 두 연석 변수 사이의 선형 종속성을 정량화하는 척도(-1 ~ 1사이값)
    - 선형 판별 분석(Linear discriminant analysis, LDA) : 범주형 변수에서 두개 이상의 레벨에소 특성을 뽑아내거나 분리하는 선형 조합을 찾는데 사용
    - 분산 분석(Analysis of Variance, ANOVA) : 하나 이상의 범주형 독립변수와 하나의 연속 종속변수를 사용하여 계산
    - 카이제곱 : 범주형 변수 그룹에 적용되는 통계 테스트, 빈도 분포를 사용해 상관 또는 연관 가능성을 결정
- 임베디드 방법
    - 라쏘 회귀(Lasso regression) : 계수 크기의 절대값에 해당하는 패널티를 추가하는 L1 정규화 수행
    - 릿지 회귀(Ridge regression) : 계수 크기의 제곱에 해당하는 패널티를 추가하는 L2 정규화 수행

### 특성공학
: 모댈을 향상하기 위해 새로운 변수 생성

- 대치
    - 결측치가 있는 행 제거
    - 수치 대치
    - 범주 대치 : 글자 대신 숫자로
- 이상치 관리
- 원핫 인코딩 : 범주대치시 숫자의 높낮이 문제가 생길수 있으니, 전부 0 1 헹렬로 만듬
- 로그 변환 : 지나치게 큰값의 정규화 방법
- 스케일링 
    - 정규화 : 0~1 사의 값으로 조정
    - 표준화(Z) : 값에 평균를 빼고 표준편차로 나눈 값
- 날짜 처리 : 날짜에 숫자로 인덱싱하여 그룹 및 특징 주기