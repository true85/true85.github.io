---
title:  "ML 기초"
layout: single
categories: code
tag: ML
toc: true
use_math: true #수학식 적용여부
author_profile: false #내 프로파일 안보이기
sidebar:
    nav: "docs" 
# search: false #검색불가
---
## 데이터
### 정형데이터 : 숫자
### 비정형 데이터
#### 텍스트
#### 이미지
눈은 색보다 밝기에 민감하다.
jpg는 용량을 줄이기 위해 밝기 정보와 Cb Cr이라는 두가지 색차 정보로 저장된다.
사진은 PNG가 좋다.

JPEG, JPG (Joint Photographic Experts Group)
: 정지 화상을 위해서 만들어진 손실 압축 방법 표준이다. 
(JPEG 를 사용하는 파일 형식들도 보통 JPEG 이미지라 불리며, .jpg, .jpeg, .jpe 등의 확장자를 사용한다. 손실 압축 형식이지만 파일크기가 작기 때문에 웹에서 널리 쓰인다. 압축률을 높이면 파일 크기는 작아지지만 이미지 품질은 더욱 떨어진다.)

PNG(Portable Network Graphics)
: 비손실 그래픽 파일 포맷, 투명성 값도 포함
(무손실 압축으로 이미지 디테일 손실이 전혀없고 고품질 이미지를 생성하지만 파일 크기는 상대적으로 다른 포맷보다 커진다.)

GIF(Graphics Interchange Format)
: PNG 포맷 이전에 개발된 비손실 그래픽 포맷 중의 하나이다.
(1개의 파일에 여러 개의 이미지를 저장할 수 있는데, 다수의 이미지를 하나의 이미지처럼 복수 처리하여 간단한 애니메이션 효과를 낼 수 있다. 웹 페이지 상에서 움직이는 그림(움짤)은 GIF 를 사용한다. )

#### 영상
코덱(Codec) = Coder + Decoder 
압축, 변환과정 Encoding 
##### 디스플레이
물감은 감산혼합 잉크
디스플레이는 가산혼합 빛
색 재현율
: 화면에서 나타낼수있는 색의 범위 , sRGB/Adobe RGB(프린터를 위한)/ DCI-P3(영화를 위한)
##### 해상도
화소
: 색을 나타내는 점
화질 = 해상도 + 명암비 + 색표현
16:9
1080p (ex. 1920*1080, 순차주사 Progressive scan)
1080i (ex. 2560*1080, 비월 주사 Interlaced sacn)
720p HD
1080p Full HD
1440p Quad HD
2160p Ultra HD
#### 음성
## 인공지능
사고나 학습등 인간이 가진 지적 능력을 컴퓨터를 통해 구현하는 기술

## 머신러닝
컴퓨터가 스스로 학습하여 인공지능의 성능을 향상시키는 기술 방법


### 지도학습(supervised learning)
: 정답 라벨로 학습

#### 회귀 (regression) 
: 유전체 선발, 시장 예측 - Random forest, Regression, 정규화 (regularization)
##### 회귀 평가 방법
- 평균제곱오차 (mean square error)
: 각 값의 오차를 제곱하여 평균한 값
- 결정계수 (coefficient of determination)
: 0~1사이 1은 오차가 없음

##### 선형회귀(linear regression)
손실을 최소화하는 파라미터 계산
$y = wx + b$
평균제곱오차 비교 : **각 데이터마다 y값 오차를 제곱하여 데이터갯수로 나누어 평균**을 해서 나온값을 비교
데이터 분포를 시각화 해서 적용가능한지 먼저 판단

**a,b로 정리하면 다항2차 방정식이** 나오는데 이것으로 최소값을 찾는다.

##### 정규화 (regularization)
과적합을 막는 방법중 하나

[손실함수](https://truman.tistory.com/164)
- 손실 함수는 실제값과 예측값의 차이(loss, cost)를 수치화해주는 함수이다. 
- 오차가 클수록 손실 함수의 값이 크고, 오차가 작을수록 손실 함수의 값이 작아진다.
- 평균제곱오차도 하나의 손실함수

정규화 항($\alpha$)을 조정하여 절댓값차이를 줄임

#### 분류 (classification) 
: 패턴 인식, 의학 진단 -  k-nn(K-nearest neighbor), SVM (Support Vector Machine), Logistic regression, Naive Bayes, Decision Tree
##### 분류 평가 방법
- 혼동 행렬 (confusion matrix)
: TN 실제 부정 데이터를 부정으로 제대로 예측, FP 실제 부정 데이터를 긍정으로 잘못 예측, FN 실제 긍정 데이터를 부정으로 잘못 예측, TP 실제 긍정 데이터를 긍정으로 제대로 예측 (행렬표시)
- 정확도 (accuracy)
: 전체 결과 중 정확하게 예측한 비율
- 정밀도 (precision)
: 실제 긍정으로 예측한 결과 중 정말 긍정인 비율
- 재현율 (recall)
: 실제 긍정인 대상을 긍정으로 제대로 예측한 비율
- F값 (F1-score)
정밀도와 재현율을 반영한 지표
- 곡선아래 면적 (area under the curve, AUC)
##### 로지스틱 회귀 (Logistic regression)
집단 각각에 속하는 확률 계산

이진분류는 시그모이드함수 활용(임계값 기준으로 1혹은 0) 또는 결정경계

손실함수로는 주로 경사하강법 이용

##### Naive Bayes
자연어 분류에 많이 이용되는 알고리즘
확율에 따른 결과를 예측하는 알고리즘
문장속 단어가 나타나는 비율, 단어 각각의 조건부 확률 등 문장을 구성하는 단어 정보를 사용해 확률 계산의 정확도를 높인다.
독립사건의 영향을 다룰때 사용한다.

##### 결정트리 (Decision Tree)
학습데이터를 나누면서 문제를 해결
데이터의 불균형을 나타내는 불순도를 수치화

앞의 두과정을 반복

#### 회귀, 분류 둘다가능한 알고리즘
SVM (Support Vector Machine)/- with kernel, random forest,K-nn(K-nearest neighbor), neural network

##### SVM (Support Vector Machine)
집단 데이터 사이의 마진을 최대화해서 좋은 결정경계를 구한다.

하드마진 : 마진안에 데이터 허용X
소프트마진 : 마진안에 데이터 허용

마진선에 있는 데이터와 안에있는 데이터를 기준으로 경계를 정한다.
이 기준 파라미터 정하는 방식으로 그리드 탐색과 랜덤 탐색이 있다.

##### SVM (Support Vector Machine) + 커널 kernel
선이 아니더라도 차원을 늘려서 경계를 만들수있다.(선형, 시그모이드, 다항(원형태), RBF 커널)

##### 랜덤 포레스트 (random forest)
결정트리를 어러번 중첩시켜 사용 다수결로 결과를 도출
다른 다양한 결과를 얻기 위해
1. 부트스트랩(bootstrap)
: 학습 데이터에서 여러번 무작위로 복원(반복) 추출
1. 특징 임의 선택
: 부트스트랩으로 만든 학습데이터에 일부 특징만을 임의로 선택 사용

##### 신경망 (neural network)
분류에 많이 사용된다.
생물의 신경망을 따서 만듬, 딥러닝에 활용
입력층 은닉층 출력층 
$y=f(w_0+w_1x_1+w_2x_2)$
$w_1,w_2$는 가중치 $w_0$는 편향 bias 를 활성화 함수($f$)에 적용 
과적합을 막기위해 학습조기종료

##### k-최근접 이웃 알고리즘(kNN, K-nearest neighbor)
학습데이트를 저장하여 입력데이터와의 거리를 계산하여 근처에 있는 k개의 점까지 데이터를 어떻게 분류하는지를 보고다수결로 결정

### 비지도 학습(unsupervised learning) 
: 정답없이 학습

#### 군집 (clustering) 
: 비슷한 성질릐 데이터를 집단으로 나누는 방법 
시장 세분화/ 추천자 시스템 - k-means, 가우시안 혼합모델(Gaussian Mixture models), Kierarchical clistering
##### k-평균알고리즘 (k-means)
1. 데이터 포인트 중 적당한 점을 집단 개수 만큼 선택해 중심으로 정함
1. 데이터 포인트와 각 궂ㅇ심 사이의 거리를 계산해 가장 가까운 중심을 해당 데이터 포인트가 속한 집단으로 정함
1. 집단 마다 데이터 포인트의 평균을 계싼하고 이를 새로운 중심으로 정함
1. 데이터 포인트 모다가 속한 집단이 변하지 않거나 더는 계산할 수 없을 때까지 반복

집단 안 제곱합으로 결과를 평가 합니다.
##### 가우시안 혼합모델(Gaussian Mixture models)
분산을 이용해 데이터 분포를 나타냄
1. 파라미터(가우스 분포 각각의 평군과 분산) 초기화
1. 데이터 포인트 각각이 갖는 가중치를 집단 마다 계산
1. 과정 2에서 얻은 가중치로 파라미터를 다시 계산
1. 과정 3에서 갱신한 평군 각각의 변화가 충분히 작아질 때까지 2,3 반복
#### 차원축소 (dimensionality reduction) 
: 특징 추출/ 빅데이터 시각화 - 주성분 분석(PCA, principal component analysis), 잠재 의미분석(LSA, latent semantic analysis), 음수 미포함 행렬 분해(NMF, non-negative matrix factorization), 잠재 디리클레 할당(LDA, latent Dirichlet allocation), 국소 선형 임베딩(LLE, local linear embedding), t-분포 확률적 임베딩(t-SNE, t-distributed stochastic neighbor embedding)

##### 주성분 분석 (PCA, principal component analysis)
상관관계가 있는 다변량 데이터를 주성분으로 간경하게 나타낼 수있다.
원본 데이터를 가공해 새로운 변수로 구성
방향과 중요도를 찾아서 가장 큰값을 주성분으로 잡는다. 두번쩨 큰값을 두번쩨 주성분으로 잡는다.

##### 잠재 의미 분석 (LSA, latent semantic analysis)
토픽 모델링 알고리즘
특이값 분해(Singular Value Decomposition, SVD)
SVD(full SVD)
절단된 SVD(Truncated SVD)
$X=UDV$
U : 단어특징의 변화 정보가 있는 행렬
D : 정보의 중요도가 있는 행렬
V : 단어 특징과 문서 변환 정보가 있는 행렬
##### 잠재 디리클레 할당(LDA, latent Dirichlet allocation)
토픽 모델링 알고리즘
1. 문서에 사용할 단어의 개수 k를 정합니다.
1. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정합니다.
1. 문서에 사영할 각 단어를 다음과 같이 정합니다.
    1. 토픽 분포에서 토픽 T를 확률적으로 고릅니다.
    1. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고릅니다.

불용어를 제외하도록 학습을 개선 가능.
##### 음수 미포함 행렬 분해(NMF, non-negative matrix factorization)
입력 데이터와 출력 데이터값이 모두 양수라는 성질이 있는 차원 축소 방법

##### 국소 선형 임베딩(LLE, local linear embedding)
높은 차워 공간에서 휘어지거나 뒤틀린 구조를 낮은 차원 공간에 단순한 구조로 나타내는 알고리즘

##### t-분포 확률적 임베딩(t-SNE, t-distributed stochastic neighbor embedding)
높은 차원의 복잡한 데이터를 2차원에 차원 축소하는 방법
### 강화 학습 (reinforcement learning)
: 시뮬레이션 반복학습, 로봇탐색/ 인공지능 게임 - K-armed bandit, 모델 기반 학습, 모델 프리 학습, 가치 함수 근사, 이미테이션 러닝

## 딥러닝
인간의 뉴런과 비슷한 인공신경망 방식으로 정보를 처리 (비지도 학습)

### CNN (Convolutional Neural Network) 합성곱 신경망
필터 혹은 커널 -> 렐루 -> 풀링

커널
: 특징추출
렐루
: 값을 증폭
풀링
: ??
### RNN (Recurrent Neural Network) 순환 신경망
앞의 값을 영향을 받아 전체적인 답을 찾아낸다.
#### LSTM (Long Short Term Memory) 
가장 유명
### GRU

### VAE

### GANs (Generative adversarial networks) 생성적 적대 신경망
스스로 발전 만들고 수정하고 발전시키는 ,,,
### RBM (Restricted Boltzmann Machine)



### 심층 강화학습

## 원핫 인코딩
하나의 값만 1 나머지 0
## 소프트 맥스
다 합쳐서 합이 1
## 과적합 피하기
- 학습셋과 테스트셋으로 분리해서 과적합인지 검증
- k 교차 검증 : 데이터셋을 k개로 나누어 학습과 테스트를 교차해서 진행
- 학습의 자동중단 : 테스트셋 오차가 줄지 않으면 학습 정지
